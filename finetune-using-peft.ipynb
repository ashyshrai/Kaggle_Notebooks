{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ashishraics/finetune-using-peft?scriptVersionId=186392857\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,DataCollatorForLanguageModeling,BitsAndBytesConfig\nfrom transformers import TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model\nimport os\nfrom torch.utils.data import DataLoader\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:15:45.305462Z","iopub.execute_input":"2024-06-23T15:15:45.306339Z","iopub.status.idle":"2024-06-23T15:15:45.311194Z","shell.execute_reply.started":"2024-06-23T15:15:45.306299Z","shell.execute_reply":"2024-06-23T15:15:45.31027Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nfoundation_model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config, device_map={\"\":0})\ndata = load_dataset(\"Nan-Do/instructional_code-search-net-python\", cache_dir=\"../working/cache\"+\"/datasets\")\nbase_model = AutoModelForCausalLM.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:15:47.634138Z","iopub.execute_input":"2024-06-23T15:15:47.634877Z","iopub.status.idle":"2024-06-23T15:15:53.746255Z","shell.execute_reply.started":"2024-06-23T15:15:47.634847Z","shell.execute_reply":"2024-06-23T15:15:53.745271Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"next(foundation_model.parameters()).dtype","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:48:29.502681Z","iopub.execute_input":"2024-06-23T13:48:29.502986Z","iopub.status.idle":"2024-06-23T13:48:29.509215Z","shell.execute_reply.started":"2024-06-23T13:48:29.50296Z","shell.execute_reply":"2024-06-23T13:48:29.508253Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"torch.float16"},"metadata":{}}]},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nfoundation_model.gradient_checkpointing_enable()\nfoundation_model = prepare_model_for_kbit_training(foundation_model)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:48:29.510563Z","iopub.execute_input":"2024-06-23T13:48:29.510871Z","iopub.status.idle":"2024-06-23T13:48:29.526775Z","shell.execute_reply.started":"2024-06-23T13:48:29.510845Z","shell.execute_reply":"2024-06-23T13:48:29.525909Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=8, \n    lora_alpha=32, \n    target_modules=[\"q_proj\",\"k_proj\"], \n    lora_dropout=0.05, \n    bias=\"none\", \n    task_type=\"CAUSAL_LM\"\n)\n\npeft_model = get_peft_model(foundation_model, config)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:48:30.735199Z","iopub.execute_input":"2024-06-23T13:48:30.735699Z","iopub.status.idle":"2024-06-23T13:48:30.840267Z","shell.execute_reply.started":"2024-06-23T13:48:30.735662Z","shell.execute_reply":"2024-06-23T13:48:30.839284Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n    \nprint_trainable_parameters(peft_model)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:48:35.878998Z","iopub.execute_input":"2024-06-23T13:48:35.879755Z","iopub.status.idle":"2024-06-23T13:48:35.889797Z","shell.execute_reply.started":"2024-06-23T13:48:35.879719Z","shell.execute_reply":"2024-06-23T13:48:35.888766Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"trainable params: 1572864 || all params: 740919296 || trainable%: 0.21228546867269063\n","output_type":"stream"}]},{"cell_type":"code","source":"shuffled_data = data[\"train\"].shuffle(seed=42)\n\ntrain_data_sample = shuffled_data.select(range(2000))\nval_data_sample = shuffled_data.select(range(2000, 2030))","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:49:49.721618Z","iopub.execute_input":"2024-06-23T13:49:49.722429Z","iopub.status.idle":"2024-06-23T13:49:49.738608Z","shell.execute_reply.started":"2024-06-23T13:49:49.722395Z","shell.execute_reply":"2024-06-23T13:49:49.737886Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:49:53.198212Z","iopub.execute_input":"2024-06-23T13:49:53.199113Z","iopub.status.idle":"2024-06-23T13:49:53.205033Z","shell.execute_reply.started":"2024-06-23T13:49:53.199083Z","shell.execute_reply":"2024-06-23T13:49:53.204138Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"#peft_model = torch.nn.DataParallel(peft_model, device_ids = [0,1]).to(device)\n#peft_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:49:54.7728Z","iopub.execute_input":"2024-06-23T13:49:54.773156Z","iopub.status.idle":"2024-06-23T13:49:54.777149Z","shell.execute_reply.started":"2024-06-23T13:49:54.773128Z","shell.execute_reply":"2024-06-23T13:49:54.776182Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples[\"INSTRUCTION\"], examples[\"RESPONSE\"], truncation=True)\n\ntrain_tokenized_data = train_data_sample.map(tokenize_function, batched=True, remove_columns=[\"INSTRUCTION\", \"RESPONSE\", \"SOURCE\"])\nval_tokenized_data = val_data_sample.map(tokenize_function, batched=True, remove_columns=[\"INSTRUCTION\", \"RESPONSE\", \"SOURCE\"])\n\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n)\n\ntrain_dataloader = DataLoader(\n    train_tokenized_data,\n    shuffle=True,\n    batch_size=1,  \n    collate_fn=data_collator,\n)\n\nval_dataloader = DataLoader(\n    val_tokenized_data,\n    batch_size=1,\n    collate_fn=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T13:49:56.782662Z","iopub.execute_input":"2024-06-23T13:49:56.783327Z","iopub.status.idle":"2024-06-23T13:49:58.123039Z","shell.execute_reply.started":"2024-06-23T13:49:56.783293Z","shell.execute_reply":"2024-06-23T13:49:58.122003Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"715db64d2ab84b43a47d79652e0069e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec4c057253a3421a8b22df854d92dc7c"}},"metadata":{}}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    overwrite_output_dir=True,\n    num_train_epochs=2, \n    per_device_train_batch_size=1, \n    logging_steps=10, \n    learning_rate=5e-5,\n    weight_decay=0.01,\n    report_to=[],  # disable reporting to external services like wandb\n    disable_tqdm=False,\n    evaluation_strategy=\"steps\", \n    eval_steps=10,  \n    gradient_accumulation_steps=4, \n    lr_scheduler_type='linear',  \n    warmup_steps=100,  \n)\n\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_tokenized_data, \n    eval_dataset=val_tokenized_data,    \n    data_collator=data_collator,\n)\n\npeft_model.config.use_cache = False \ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T14:01:58.042884Z","iopub.execute_input":"2024-06-23T14:01:58.043752Z","iopub.status.idle":"2024-06-23T14:57:50.791941Z","shell.execute_reply.started":"2024-06-23T14:01:58.043717Z","shell.execute_reply":"2024-06-23T14:57:50.79106Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 55:49, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.860100</td>\n      <td>1.761057</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.947000</td>\n      <td>1.760111</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.851000</td>\n      <td>1.758253</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.020500</td>\n      <td>1.754841</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.176900</td>\n      <td>1.748510</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.931600</td>\n      <td>1.737558</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.874400</td>\n      <td>1.721354</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.794500</td>\n      <td>1.698768</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.847200</td>\n      <td>1.670522</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.683000</td>\n      <td>1.643397</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.742500</td>\n      <td>1.621038</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.643300</td>\n      <td>1.604410</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.632300</td>\n      <td>1.589599</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.636400</td>\n      <td>1.575162</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.674500</td>\n      <td>1.560719</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.570400</td>\n      <td>1.546427</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.624200</td>\n      <td>1.532944</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.671200</td>\n      <td>1.517705</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.562900</td>\n      <td>1.502842</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.609900</td>\n      <td>1.491130</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.673200</td>\n      <td>1.478656</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.529200</td>\n      <td>1.467095</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.487600</td>\n      <td>1.456908</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.506700</td>\n      <td>1.448550</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.646800</td>\n      <td>1.440323</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.481700</td>\n      <td>1.435588</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.512500</td>\n      <td>1.431499</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.424000</td>\n      <td>1.425242</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.471900</td>\n      <td>1.420538</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.512100</td>\n      <td>1.417276</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.531800</td>\n      <td>1.414025</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.529600</td>\n      <td>1.409795</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.384300</td>\n      <td>1.405669</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.405900</td>\n      <td>1.402263</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.476000</td>\n      <td>1.398547</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.552300</td>\n      <td>1.395145</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.373100</td>\n      <td>1.392491</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.513600</td>\n      <td>1.390420</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.401500</td>\n      <td>1.386754</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.431400</td>\n      <td>1.384469</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.461100</td>\n      <td>1.382187</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.533400</td>\n      <td>1.379018</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>1.433200</td>\n      <td>1.376194</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.421800</td>\n      <td>1.373848</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.303700</td>\n      <td>1.371385</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.369800</td>\n      <td>1.369446</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.417500</td>\n      <td>1.368226</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.385800</td>\n      <td>1.367952</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.423100</td>\n      <td>1.367996</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.406500</td>\n      <td>1.366736</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>1.405700</td>\n      <td>1.365571</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.384600</td>\n      <td>1.365465</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>1.406500</td>\n      <td>1.364426</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.416700</td>\n      <td>1.362562</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.266800</td>\n      <td>1.361758</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>1.413800</td>\n      <td>1.361604</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>1.310300</td>\n      <td>1.361184</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>1.300700</td>\n      <td>1.360661</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>1.270200</td>\n      <td>1.359810</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.294100</td>\n      <td>1.359383</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>1.346500</td>\n      <td>1.358660</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>1.363300</td>\n      <td>1.357240</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>1.307500</td>\n      <td>1.356316</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>1.409900</td>\n      <td>1.355812</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.413600</td>\n      <td>1.355598</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>1.379500</td>\n      <td>1.354924</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>1.306800</td>\n      <td>1.354682</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>1.280300</td>\n      <td>1.354008</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>1.388300</td>\n      <td>1.353806</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.333600</td>\n      <td>1.353460</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>1.207100</td>\n      <td>1.352739</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>1.288400</td>\n      <td>1.352006</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>1.472900</td>\n      <td>1.351660</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>1.329100</td>\n      <td>1.351292</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.405400</td>\n      <td>1.350899</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>1.283100</td>\n      <td>1.350773</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>1.382600</td>\n      <td>1.350803</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>1.277800</td>\n      <td>1.350566</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>1.361700</td>\n      <td>1.350203</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.403800</td>\n      <td>1.349751</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>1.366700</td>\n      <td>1.349712</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>1.377400</td>\n      <td>1.349642</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>1.308900</td>\n      <td>1.349721</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>1.370500</td>\n      <td>1.349703</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>1.468500</td>\n      <td>1.349914</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>1.393200</td>\n      <td>1.349838</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>1.330300</td>\n      <td>1.349719</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>1.436700</td>\n      <td>1.349533</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>1.407800</td>\n      <td>1.349543</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.341000</td>\n      <td>1.349339</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>1.340800</td>\n      <td>1.349059</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>1.346600</td>\n      <td>1.348846</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>1.284700</td>\n      <td>1.348657</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>1.343900</td>\n      <td>1.348619</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>1.431600</td>\n      <td>1.348665</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>1.373100</td>\n      <td>1.348719</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>1.392600</td>\n      <td>1.348767</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>1.329300</td>\n      <td>1.348772</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>1.267600</td>\n      <td>1.348768</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.305600</td>\n      <td>1.348752</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=1.4698223133087158, metrics={'train_runtime': 3352.3336, 'train_samples_per_second': 1.193, 'train_steps_per_second': 0.298, 'total_flos': 9098449199185920.0, 'train_loss': 1.4698223133087158, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"messages=[\n    { 'role': 'user', 'content': \"How do I change my pandas dataframe column name\"}\n]\ninputs = tokenizer.apply_chat_template(messages, \n                                       add_generation_prompt=True, \n                                       return_tensors=\"pt\").to(foundation_model.device)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:12:02.84769Z","iopub.execute_input":"2024-06-23T15:12:02.848536Z","iopub.status.idle":"2024-06-23T15:12:02.864178Z","shell.execute_reply.started":"2024-06-23T15:12:02.848504Z","shell.execute_reply":"2024-06-23T15:12:02.863266Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"%%time\nbase_model.to(device)\noutputs = base_model.generate(inputs, max_new_tokens=200, do_sample=False,\n                         top_k=5, top_p=0.9, num_return_sequences=1, \n                         eos_token_id=tokenizer.eos_token_id,\n                        early_stopping=True)\n\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:16:35.529124Z","iopub.execute_input":"2024-06-23T15:16:35.529819Z","iopub.status.idle":"2024-06-23T15:16:42.208023Z","shell.execute_reply.started":"2024-06-23T15:16:35.529785Z","shell.execute_reply":"2024-06-23T15:16:42.207085Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"You can change the name of a column in a pandas DataFrame using the `rename` function. Here's an example:\n\n```python\nimport pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n\n# Rename a column\ndf = df.rename(columns={'A': 'New_A'})\n\nprint(df)\n```\n\nIn this example, the column 'A' in the DataFrame will be renamed to 'New_A'.\n\nIf you want to rename multiple columns, you can pass a dictionary where the keys are the old names and the values are the new names:\n\n```python\ndf = df.rename(columns={'A': 'New_A', 'B': 'New_B'\nCPU times: user 6.52 s, sys: 160 ms, total: 6.68 s\nWall time: 6.67 s\n","output_type":"stream"}]},{"cell_type":"code","source":"peft_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:16:11.162964Z","iopub.execute_input":"2024-06-23T15:16:11.163349Z","iopub.status.idle":"2024-06-23T15:16:11.190027Z","shell.execute_reply.started":"2024-06-23T15:16:11.163318Z","shell.execute_reply":"2024-06-23T15:16:11.189035Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32256, 2048)\n        (layers): ModuleList(\n          (0-23): 24 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n              (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=2048, out_features=5504, bias=False)\n              (up_proj): Linear4bit(in_features=2048, out_features=5504, bias=False)\n              (down_proj): Linear4bit(in_features=5504, out_features=2048, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm()\n            (post_attention_layernorm): LlamaRMSNorm()\n          )\n        )\n        (norm): LlamaRMSNorm()\n      )\n      (lm_head): Linear(in_features=2048, out_features=32256, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"%%time\noutputs = peft_model.generate(inputs, max_new_tokens=200, do_sample=False,\n                         top_k=5, top_p=0.9, num_return_sequences=1, \n                         eos_token_id=tokenizer.eos_token_id,\n                        early_stopping=True)\n\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-06-23T15:16:15.029482Z","iopub.execute_input":"2024-06-23T15:16:15.030112Z","iopub.status.idle":"2024-06-23T15:16:28.338812Z","shell.execute_reply.started":"2024-06-23T15:16:15.03008Z","shell.execute_reply":"2024-06-23T15:16:28.337705Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:562: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"You can change the name of a column in a pandas DataFrame using the `rename()` function. Here's an example:\n\n```python\nimport pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n\n# Rename a column\ndf = df.rename(columns={'A': 'New_A', 'B': 'New_B'})\n```\n\nIn this example, the column 'A' in the DataFrame is renamed to 'New_A', and the column 'B' is renamed to 'New_B'.\n\nYou can also rename multiple columns at once:\n\n```python\ndf = df.rename(columns={'A': 'New_A', 'B': 'New_B', 'C\nCPU times: user 13.3 s, sys: 7.79 ms, total: 13.3 s\nWall time: 13.3 s\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}