{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom tqdm import tqdm\nimport joblib\nimport gc\nimport time\nimport copy\nimport os\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW, RAdam\n\nimport transformers\ntransformers.logging.set_verbosity_error()\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom transformers import AutoTokenizer,AutoModel,AutoModelForSequenceClassification,AutoConfig\nfrom transformers import DataCollatorWithPadding\n\nfrom transformers import (get_cosine_schedule_with_warmup,\n                         get_linear_schedule_with_warmup,\n                         get_cosine_with_hard_restarts_schedule_with_warmup)\nfrom torch.utils.data import DataLoader\nfrom torch.optim import lr_scheduler\n\nfrom sklearn import datasets\nfrom sklearn import model_selection\n\npd.set_option('max_columns',None)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:28.521159Z","iopub.execute_input":"2022-09-15T13:16:28.521739Z","iopub.status.idle":"2022-09-15T13:16:35.605040Z","shell.execute_reply.started":"2022-09-15T13:16:28.521611Z","shell.execute_reply":"2022-09-15T13:16:35.604110Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"in_=torch.tensor([1.,2.,3.])\nin2_=torch.tensor([1.,3.,2.])\n\nt_=torch.tensor([2.,3.,3.])\n\n\nloss = nn.KLDivLoss(reduction = 'mean')\nloss2 = nn.MSELoss()\nloss3=nn.SmoothL1Loss()\n\nprint(loss(in_, t_),loss(in2_, t_))\nprint(loss2(in_, t_),loss2(in2_, t_))\nprint(loss3(in_, t_),loss3(in2_, t_))","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:35.606942Z","iopub.execute_input":"2022-09-15T13:16:35.608119Z","iopub.status.idle":"2022-09-15T13:16:35.658591Z","shell.execute_reply.started":"2022-09-15T13:16:35.608079Z","shell.execute_reply":"2022-09-15T13:16:35.657597Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"tensor(-3.0073) tensor(-3.0073)\ntensor(0.6667) tensor(0.6667)\ntensor(0.3333) tensor(0.3333)\n","output_type":"stream"}]},{"cell_type":"code","source":"class CONFIG:\n    seed=42\n    train_path='../input/fp3-data/train_folds.csv'\n    test_path='../input/feedback-prize-english-language-learning/test.csv'\n    chkpt='microsoft/deberta-v3-base'\n    hf_model='hf_model'\n    trainable=True\n    max_length=512\n    n_fold=5\n    epochs=3\n    train_batch_size=8\n    val_batch_size=16\n    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    n_accumulate= 1\n    learning_rate= 1e-5\n    weight_decay= 1e-3\n    eps= 1e-6\n    betas= [0.9, 0.999]\n    min_lr= 1e-7\n    T_max= 500\n    replace_newline='[SEP]'\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:35.660017Z","iopub.execute_input":"2022-09-15T13:16:35.660354Z","iopub.status.idle":"2022-09-15T13:16:35.730438Z","shell.execute_reply.started":"2022-09-15T13:16:35.660319Z","shell.execute_reply":"2022-09-15T13:16:35.729465Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG.seed)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:35.734369Z","iopub.execute_input":"2022-09-15T13:16:35.734667Z","iopub.status.idle":"2022-09-15T13:16:35.743027Z","shell.execute_reply.started":"2022-09-15T13:16:35.734640Z","shell.execute_reply":"2022-09-15T13:16:35.742111Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_train=pd.read_csv(CONFIG.train_path)\ndf_train['full_text']=df_train['full_text'].apply(lambda x:x.strip().lstrip().rstrip())\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:35.744231Z","iopub.execute_input":"2022-09-15T13:16:35.745353Z","iopub.status.idle":"2022-09-15T13:16:35.972172Z","shell.execute_reply.started":"2022-09-15T13:16:35.745318Z","shell.execute_reply":"2022-09-15T13:16:35.971212Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        text_id                                          full_text  cohesion  \\\n0  0016926B079C  I think that students would benefit from learn...       3.5   \n1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n3  003885A45F42  The best time in life is when you become yours...       4.5   \n4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n\n   syntax  vocabulary  phraseology  grammar  conventions  kfold  \n0     3.5         3.0          3.0      4.0          3.0      1  \n1     2.5         3.0          2.0      2.0          2.5      0  \n2     3.5         3.0          3.0      3.0          2.5      4  \n3     4.5         4.5          4.5      4.0          5.0      3  \n4     3.0         3.0          3.0      2.5          2.5      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n      <th>kfold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0016926B079C</td>\n      <td>I think that students would benefit from learn...</td>\n      <td>3.5</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0022683E9EA5</td>\n      <td>When a problem is a change you have to let it ...</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00299B378633</td>\n      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003885A45F42</td>\n      <td>The best time in life is when you become yours...</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0049B1DF5CCC</td>\n      <td>Small act of kindness can impact in other peop...</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# def clean_text(text):\n#     if CONFIG.replace_newline:\n#         text = text.replace('\\n\\n-\\n\\n',CONFIG.replace_newline)\n#         text = text.replace('\\n\\n',CONFIG.replace_newline)\n#         text=text.replace('\\r\\n\\r\\n',CONFIG.replace_newline)\n#         text=text.replace('\\r\\n',CONFIG.replace_newline)\n#         text=text.replace('STUDENT_NAME','student')\n#         text=text.replace('SCHOOL_NAME','school')\n#         text=text.replace('Generic_Name','Paul')\n#         text=text.replace('TEACHER_NAME','teacher')\n#     return text\n\n# df_train['full_text']=df_train['full_text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:35.973653Z","iopub.execute_input":"2022-09-15T13:16:35.974224Z","iopub.status.idle":"2022-09-15T13:16:35.978810Z","shell.execute_reply.started":"2022-09-15T13:16:35.974188Z","shell.execute_reply":"2022-09-15T13:16:35.977878Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# all_special_tokens = []\n# if CONFIG.replace_newline:\n#     all_special_tokens.append(CONFIG.replace_newline)\n    \ntokenizer = AutoTokenizer.from_pretrained(CONFIG.chkpt,\n                                          use_fast=True, \n#                                           additional_special_tokens=all_special_tokens, \n                                          return_special_tokens_mask=True)\n\n# tokenizer.model_max_length = CONFIG.max_length\ntokenizer.save_pretrained(f'{CONFIG.hf_model}')","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:35.980187Z","iopub.execute_input":"2022-09-15T13:16:35.980802Z","iopub.status.idle":"2022-09-15T13:16:45.308200Z","shell.execute_reply.started":"2022-09-15T13:16:35.980768Z","shell.execute_reply":"2022-09-15T13:16:45.307193Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"327f5d89117d49989d1d3009d5178c12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e286cf3a0ed2445fb48dce0fe702842e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fddcb34783841288b46a86a5178fe1f"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"('hf_model/tokenizer_config.json',\n 'hf_model/special_tokens_map.json',\n 'hf_model/spm.model',\n 'hf_model/added_tokens.json',\n 'hf_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"class MakeTorchDataset(torch.utils.data.Dataset):\n    def __init__(self,df,tokenizer):\n        self.df=df\n        self.tokenizer=tokenizer\n        self.max_length=CONFIG.max_length\n        self.full_text=self.df['full_text'].values\n        self.labels=self.df[['cohesion', 'syntax', 'vocabulary','phraseology', 'grammar', 'conventions']].values.tolist()\n    def __len__(self):\n        return len(self.df)\n\n  \n    def __getitem__(self,idx):\n\n        text=self.tokenizer.encode_plus(self.full_text[idx],\n                        add_special_tokens=True,\n                        truncation=True,\n                        padding='max_length',\n                        max_length=self.max_length,\n                        )\n\n        ids=text['input_ids']\n        masks=text['attention_mask']\n        masks_sep_cls=[1 if i in [self.tokenizer.cls_token_id,self.tokenizer.sep_token_id] else 0 for i in ids]\n        \n        return {\n            'input_ids':ids,\n            'attention_masks':masks,\n            'masks_sep_cls':masks_sep_cls,\n            'labels':self.labels[idx]\n        }\n","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:45.309652Z","iopub.execute_input":"2022-09-15T13:16:45.310213Z","iopub.status.idle":"2022-09-15T13:16:45.318720Z","shell.execute_reply.started":"2022-09-15T13:16:45.310175Z","shell.execute_reply":"2022-09-15T13:16:45.317790Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class MeanPooling(torch.nn.Module):\n    def __init__(self):\n        super(MeanPooling,self).__init__()\n    \n    def forward(self,last_hidden_state,attention_mask,masks_sep_cls):\n        input_mask_expanded=attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:45.320273Z","iopub.execute_input":"2022-09-15T13:16:45.320966Z","iopub.status.idle":"2022-09-15T13:16:45.336141Z","shell.execute_reply.started":"2022-09-15T13:16:45.320903Z","shell.execute_reply":"2022-09-15T13:16:45.335269Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class FeedModel(torch.nn.Module):\n    def __init__(self):\n        super(FeedModel,self).__init__()\n        \n        self.CONFIG=CONFIG    \n        if self.CONFIG.trainable:\n            self.chkpt=self.CONFIG.chkpt\n            self.config=AutoConfig.from_pretrained(self.chkpt)\n        else:\n            self.chkpt=self.CONFIG.hf_model\n            self.config=AutoConfig.from_pretrained(self.CONFIG.hf_model)\n            \n        self.config.hidden_dropout = 0.\n        self.config.hidden_dropout_prob = 0.\n        self.config.attention_dropout = 0.\n        self.config.attention_probs_dropout_prob = 0.\n            \n#         self.config.max_position_embeddings=self.CONFIG['max_length']\n#         self.config.position_biased_input=True #default is true\n#         self.config.relative_attention=False#default is false\n\n        self.mean_pooler=MeanPooling()\n#         self.weight_layer_pooler=WeightedLayerPooling(num_hidden_layers=self.config.num_hidden_layers,\n#                                                      layer_start=6)\n#         self.bilstm = nn.LSTM(self.config.hidden_size, (self.config.hidden_size) // 2, num_layers=2, \n#                               dropout=self.config.hidden_dropout_prob, batch_first=True,\n#                               bidirectional=True)\n        \n        self.dropout1=nn.Dropout(0.1)\n        self.dropout2=nn.Dropout(0.2)\n        self.dropout3=nn.Dropout(0.3)\n        self.dropout4=nn.Dropout(0.4)\n        self.dropout5=nn.Dropout(0.5)\n        \n        self.fc=nn.Linear(self.config.hidden_size,6)\n        self.model=self.create_model()\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    \n\n    def create_model(self):\n        if self.CONFIG.trainable:\n            model=AutoModel.from_pretrained(self.chkpt,output_hidden_states=True)\n#             import torch.utils.checkpoint\n#             model.gradient_checkpointing_enable()\n            model.save_pretrained(CONFIG.hf_model)\n        else:\n            model=AutoModel.from_pretrained(CONFIG.hf_model)\n        return model\n\n    def forward(self,input_ids,attention_mask,masks_sep_cls):\n        \n#         # simple CLS\n#         transformer_out=self.model(input_ids,attention_mask)\n#         transformer_out = transformer_out[0][:, 0, :]\n        \n        #simple mean pooling\n        transformer_out=self.model(input_ids,attention_mask)['last_hidden_state']\n        transformer_out=self.mean_pooler(transformer_out,attention_mask,masks_sep_cls)\n        \n#        #weighted layer mean pooling foloowed by mean pooling\n#         transformer_out=self.model(input_ids,attention_mask)['hidden_states']\n#         transformer_out=torch.stack(transformer_out)\n#         transformer_out=self.weight_layer_pooler(transformer_out)\n#         transformer_out=self.mean_pooler(transformer_out,attention_mask)\n        \n#         transformer_out=self.dropout1(transformer_out)\n        logits=self.fc(transformer_out)\n        \n\n#         logits1=self.fc(self.dropout1(transformer_out))\n#         logits2=self.fc(self.dropout2(transformer_out))\n#         logits3=self.fc(self.dropout3(transformer_out))\n#         logits4=self.fc(self.dropout4(transformer_out))\n#         logits5=self.fc(self.dropout5(transformer_out))\n#         logits=(logits1+logits2+logits3+logits4+logits5)/5\n        \n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:45.340393Z","iopub.execute_input":"2022-09-15T13:16:45.341252Z","iopub.status.idle":"2022-09-15T13:16:45.357042Z","shell.execute_reply.started":"2022-09-15T13:16:45.341218Z","shell.execute_reply":"2022-09-15T13:16:45.356106Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n        output[\"attention_masks\"] = [sample[\"attention_masks\"] for sample in batch]\n        output[\"masks_sep_cls\"] = [sample[\"masks_sep_cls\"] for sample in batch]\n        output[\"labels\"] = [sample[\"labels\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n            output[\"attention_masks\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_masks\"]]\n            output[\"masks_sep_cls\"] = [s + (batch_max - len(s)) * [0] for s in output[\"masks_sep_cls\"]]\n        else:\n            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"input_ids\"]]\n            output[\"attention_masks\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_masks\"]]\n            output[\"masks_sep_cls\"] = [s + (batch_max - len(s)) * [0] for s in output[\"masks_sep_cls\"]]\n\n        # convert to tensors\n        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n        output[\"attention_masks\"] = torch.tensor(output[\"attention_masks\"], dtype=torch.long)\n        output[\"masks_sep_cls\"] = torch.tensor(output[\"masks_sep_cls\"], dtype=torch.long)\n        output[\"labels\"] = torch.tensor(output[\"labels\"], dtype=torch.float)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:45.358157Z","iopub.execute_input":"2022-09-15T13:16:45.358742Z","iopub.status.idle":"2022-09-15T13:16:45.373775Z","shell.execute_reply.started":"2022-09-15T13:16:45.358706Z","shell.execute_reply":"2022-09-15T13:16:45.372945Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"collate_fn = Collate(tokenizer)\n\ndef get_data_loader(df,fold,tokenizer,train_batch_size,val_batch_size,max_length):\n    \n    df_train=df[df['kfold']!=fold].reset_index(drop=True)\n    df_val=df[df['kfold']==fold].reset_index(drop=True)\n    \n    train_torch_dataset=MakeTorchDataset(df_train,\n                                  tokenizer=tokenizer)\n\n    val_torch_dataset=MakeTorchDataset(df_val,\n                                tokenizer=tokenizer)\n    \n    \n    train_loader = DataLoader(train_torch_dataset, \n                            batch_size=train_batch_size,\n                            collate_fn=collate_fn, \n                            shuffle=True,\n                            drop_last=True,\n                           pin_memory=True,\n                           num_workers=2)\n\n\n    val_loader = DataLoader(val_torch_dataset,\n                          batch_size=val_batch_size,\n                          collate_fn=collate_fn,\n                          shuffle=False,  \n                         pin_memory=True,\n                         num_workers=2,)\n    return train_loader,val_loader","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:45.375112Z","iopub.execute_input":"2022-09-15T13:16:45.375753Z","iopub.status.idle":"2022-09-15T13:16:45.389267Z","shell.execute_reply.started":"2022-09-15T13:16:45.375719Z","shell.execute_reply":"2022-09-15T13:16:45.388240Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"t=df_train.head(16).copy()\ndata=MakeTorchDataset(t,tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:45.390437Z","iopub.execute_input":"2022-09-15T13:16:45.393488Z","iopub.status.idle":"2022-09-15T13:16:45.408135Z","shell.execute_reply.started":"2022-09-15T13:16:45.393461Z","shell.execute_reply":"2022-09-15T13:16:45.407158Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(model,optimizer,scheduler,dataloader,device,epoch,fold):\n    model.train()\n\n    dataset_size=0\n    running_loss=0\n    running_fp_metr=0\n    preds=[]\n    labels_all=[]\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n\n    for i,data in bar:\n        input_ids=data['input_ids'].to(device)\n        attention_masks=data['attention_masks'].to(device)\n        masks_sep_cls=data['masks_sep_cls'].to(device)\n        labels=data['labels'].to(device)\n\n        batch_size=input_ids.size(0)\n\n        outputs=model(input_ids=input_ids,attention_mask=attention_masks,masks_sep_cls=masks_sep_cls)\n        loss=criterion(outputs,labels)\n        loss = loss / CONFIG.n_accumulate\n        loss.backward()\n    \n        if (i + 1) % CONFIG.n_accumulate == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n\n        running_loss+=(loss.item()*batch_size)\n        dataset_size+=batch_size\n        epoch_loss=running_loss/dataset_size\n        preds.append(outputs.detach().cpu().numpy())\n        labels_all.append(labels.detach().cpu().numpy())\n#         fp_metr=monitor_metrics(outputs,labels)\n#         running_fp_metr+=(fp_metr.item()*batch_size)\n#         epoch_fp_metr=running_fp_metr/dataset_size\n\n        bar.set_postfix(Epoch=epoch,Fold=fold, Train_Loss=epoch_loss,LR=optimizer.param_groups[0]['lr'])\n    \n    predictions = np.concatenate(preds)   \n    labels_all = np.concatenate(labels_all) \n    gc.collect()\n    return epoch_loss,predictions,labels_all","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:45.409155Z","iopub.execute_input":"2022-09-15T13:16:45.411513Z","iopub.status.idle":"2022-09-15T13:16:45.423087Z","shell.execute_reply.started":"2022-09-15T13:16:45.411480Z","shell.execute_reply":"2022-09-15T13:16:45.422190Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_one_epoch(model,dataloader,device,epoch,fold):\n\n    model.eval()\n\n    running_loss=0\n    running_fp_metr=0\n    dataset_size=0\n    preds=[]\n    labels_all=[]\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n\n    for i, data in bar:\n\n        input_ids=data['input_ids'].to(device)\n        attention_masks=data['attention_masks'].to(device)\n        masks_sep_cls=data['masks_sep_cls'].to(device)\n        labels=data['labels'].to(device)\n        batch_size=input_ids.size(0)\n\n        outputs=model(input_ids=input_ids,attention_mask=attention_masks,masks_sep_cls=masks_sep_cls)\n        \n        loss=criterion(outputs,labels)\n        running_loss+=(loss.item()*batch_size)\n        dataset_size+=batch_size\n        epoch_loss=running_loss/dataset_size\n        \n        preds.append(outputs.detach().cpu().numpy())\n        labels_all.append(labels.detach().cpu().numpy())\n#         fp_metr=monitor_metrics(outputs,labels)\n#         running_fp_metr+=(fp_metr.item()*batch_size)\n#         epoch_fp_metr=running_fp_metr/dataset_size\n        \n        bar.set_postfix(Epoch=epoch,Folf=fold, Val_Loss=epoch_loss)\n    gc.collect()\n    predictions = np.concatenate(preds)\n    labels_all = np.concatenate(labels_all)\n    return epoch_loss,predictions,labels_all","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:45.424432Z","iopub.execute_input":"2022-09-15T13:16:45.425446Z","iopub.status.idle":"2022-09-15T13:16:45.438084Z","shell.execute_reply.started":"2022-09-15T13:16:45.425407Z","shell.execute_reply":"2022-09-15T13:16:45.437137Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import copy\ndef start_training(nepochs,fold,model,optimizer,scheduler,train_loader,val_loader,device):\n    best_val_loss=np.inf\n    patience=0\n    best_model_wts = copy.deepcopy(model.state_dict())\n    for epoch in range(nepochs):\n        start=time.time()\n        gc.collect()\n\n        train_epoch_loss,preds_tr,labels_tr=train_one_epoch(model,\n                                      optimizer=optimizer,\n                                      scheduler=scheduler,\n                                      dataloader=train_loader,\n                                      device=device,\n                                      epoch=epoch,\n                                     fold=fold)\n\n        val_epoch_loss,preds_va,labels_va=valid_one_epoch(model,\n                                  dataloader=val_loader,\n                                  device=device,\n                                  epoch=epoch,\n                                  fold=fold)\n        end=time.time()\n        \n#         train_labels=train_loader['labels']\n#         val_labels=val_loader['labels']\n        \n        fp_metric_tr=get_score(preds_tr, labels_tr)\n        fp_metric_va=get_score(preds_va, labels_va)\n        \n        print(f\"Fold {fold} Epoch {epoch}: \\\n        train_loss {train_epoch_loss:.6f} \\\n        val_loss {val_epoch_loss:.6f}  \\\n        fp_metric_tr {np.round(fp_metric_tr,4)} \\\n        fp_metric_va  {np.round(fp_metric_va,4)} \\\n        time {(end-start):.1f} seconds\")\n\n        if val_epoch_loss<=best_val_loss:\n            print(f\"Validation Loss Improved ({best_val_loss} ---> {val_epoch_loss})\")\n            best_val_loss=val_epoch_loss\n            PATH = f\"{CONFIG.epochs}-{fold}.bin\"\n            best_model_wts = copy.deepcopy(model.state_dict())\n            torch.save(model.state_dict(), PATH)\n            print(f\"Model Saved\")\n        else:\n            patience+=1\n\n        print()\n    \n     # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:45.439484Z","iopub.execute_input":"2022-09-15T13:16:45.440139Z","iopub.status.idle":"2022-09-15T13:16:45.454587Z","shell.execute_reply.started":"2022-09-15T13:16:45.440105Z","shell.execute_reply":"2022-09-15T13:16:45.453660Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def get_optimizer_scheduler(model,num_training_steps):\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": CONFIG.weight_decay,\n            },\n            {\n                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        \n        \n        opt = AdamW(optimizer_parameters,\n                    lr=CONFIG.learning_rate,\n                    weight_decay=CONFIG.weight_decay,\n#                     betas=CONFIG.betas,\n#                     eps=CONFIG.eps\n                   )\n    \n        \n        opt2=RAdam(optimizer_parameters,\n                   lr=CONFIG.learning_rate,\n                    betas=CONFIG.betas,\n                    eps=CONFIG.eps)\n        \n        sch1 = get_linear_schedule_with_warmup(\n            opt,\n            num_training_steps=num_training_steps,\n            num_warmup_steps=0.1*num_training_steps,\n            last_epoch=-1,\n        )\n        \n        sch2 = get_cosine_schedule_with_warmup(\n            opt,\n            num_warmup_steps=0.1*num_training_steps,#0.1*num_training_steps\n            num_training_steps=num_training_steps,\n            num_cycles=0.5,\n            last_epoch=-1,\n        )\n        \n        sch3= get_cosine_with_hard_restarts_schedule_with_warmup(\n            opt,\n            num_warmup_steps=0,\n            num_training_steps=num_training_steps,\n            num_cycles=3,\n            last_epoch=-1\n        )\n        \n        sch4 = lr_scheduler.CosineAnnealingLR(opt,\n                                            T_max=CONFIG.T_max, \n                                            eta_min=CONFIG.min_lr)\n        \n        return opt, sch1","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:45.457121Z","iopub.execute_input":"2022-09-15T13:16:45.458679Z","iopub.status.idle":"2022-09-15T13:16:45.476129Z","shell.execute_reply.started":"2022-09-15T13:16:45.458642Z","shell.execute_reply":"2022-09-15T13:16:45.475317Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\ndef criterion(outputs, targets):\n#     loss_fct = nn.MSELoss()\n    loss_fct=nn.SmoothL1Loss(reduction='mean', beta=1)#slight better lb score due to  beta=0.51\n    loss = loss_fct(outputs, targets)\n    return loss\n\ndef monitor_metrics(outputs, targets):\n#     device = targets.get_device()\n#     outputs = outputs.detach().cpu().numpy()\n#     targets = targets.detach().cpu().numpy()\n    mcrmse = []\n    for i in range(6):\n        mcrmse.append(\n            metrics.mean_squared_error(\n                targets[:, i],\n                outputs[:, i],\n                squared=False,\n            ),\n        )\n        \n    return np.mean(mcrmse)\n\ndef get_score(outputs, targets):\n    mcrmse_score = monitor_metrics(outputs, targets)\n    return mcrmse_score","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:45.477677Z","iopub.execute_input":"2022-09-15T13:16:45.478333Z","iopub.status.idle":"2022-09-15T13:16:45.490583Z","shell.execute_reply.started":"2022-09-15T13:16:45.478282Z","shell.execute_reply":"2022-09-15T13:16:45.489630Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"temp_df=df_train.sample(n=300).copy()","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:45.493516Z","iopub.execute_input":"2022-09-15T13:16:45.494394Z","iopub.status.idle":"2022-09-15T13:16:45.505733Z","shell.execute_reply.started":"2022-09-15T13:16:45.494358Z","shell.execute_reply":"2022-09-15T13:16:45.504690Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"device=CONFIG.device\n\nfor fold in range(5):\n\n    train_loader,val_loader=get_data_loader(df_train,#temp_df\n                                            fold=fold,\n                                            tokenizer=tokenizer,\n                                            train_batch_size=CONFIG.train_batch_size,\n                                            val_batch_size=CONFIG.val_batch_size,\n                                            max_length=CONFIG.max_length)\n    model=FeedModel()\n    \n    \n    \n    num_training_steps = int(CONFIG.epochs * len(train_loader)/ CONFIG.n_accumulate)\n    optimizer,scheduler=get_optimizer_scheduler(model,num_training_steps)\n    \n    model=model.to(device)\n \n    model=start_training(CONFIG.epochs,\n                   fold,\n                   model,\n                   optimizer,\n                   scheduler,\n                   train_loader,val_loader,\n                   device)\n    \n    del model\n    gc.collect()\n    \n    break","metadata":{"execution":{"iopub.status.busy":"2022-09-15T13:16:45.507047Z","iopub.execute_input":"2022-09-15T13:16:45.508024Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/354M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1b7d497594344409a76a76d221671a1"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 391/391 [04:22<00:00,  1.49it/s, Epoch=0, Fold=0, LR=7.41e-6, Train_Loss=0.419]\n100%|██████████| 49/49 [00:21<00:00,  2.31it/s, Epoch=0, Folf=0, Val_Loss=0.135]\n","output_type":"stream"},{"name":"stdout","text":"Fold 0 Epoch 0:         train_loss 0.418695         val_loss 0.135280          fp_metric_tr 1.1304999589920044         fp_metric_va  0.5216000080108643         time 284.8 seconds\nValidation Loss Improved (inf ---> 0.13527973628867312)\nModel Saved\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [04:21<00:00,  1.50it/s, Epoch=1, Fold=0, LR=3.7e-6, Train_Loss=0.109] \n100%|██████████| 49/49 [00:21<00:00,  2.31it/s, Epoch=1, Folf=0, Val_Loss=0.166]\n","output_type":"stream"},{"name":"stdout","text":"Fold 0 Epoch 1:         train_loss 0.109352         val_loss 0.166114          fp_metric_tr 0.46869999170303345         fp_metric_va  0.5810999870300293         time 283.3 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [04:21<00:00,  1.50it/s, Epoch=2, Fold=0, LR=0, Train_Loss=0.1]         \n100%|██████████| 49/49 [00:21<00:00,  2.31it/s, Epoch=2, Folf=0, Val_Loss=0.149]\n","output_type":"stream"},{"name":"stdout","text":"Fold 0 Epoch 2:         train_loss 0.100215         val_loss 0.148652          fp_metric_tr 0.448199987411499         fp_metric_va  0.5480999946594238         time 283.2 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [04:21<00:00,  1.50it/s, Epoch=0, Fold=1, LR=7.41e-6, Train_Loss=0.408]\n100%|██████████| 49/49 [00:21<00:00,  2.31it/s, Epoch=0, Folf=1, Val_Loss=0.21] \n","output_type":"stream"},{"name":"stdout","text":"Fold 1 Epoch 0:         train_loss 0.408492         val_loss 0.209863          fp_metric_tr 1.111899971961975         fp_metric_va  0.6536999940872192         time 283.1 seconds\nValidation Loss Improved (inf ---> 0.2098631973391444)\nModel Saved\n\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 372/391 [04:09<00:12,  1.50it/s, Epoch=1, Fold=1, LR=3.87e-6, Train_Loss=0.112]","output_type":"stream"}]},{"cell_type":"code","source":"#0.113 2nd valdata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}